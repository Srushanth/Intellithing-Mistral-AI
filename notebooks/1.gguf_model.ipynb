{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"../model_gguf/\", \n",
    "                                           model_file=\"mistral-7b-instruct-v0.1.Q5_K_M.gguf\", \n",
    "                                           model_type=\"mistral\", \n",
    "                                           #gpu_layers=50\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 years old. My interests are playing video games, programming and watching movies. I have two best friends named Rohan and Sagar.\n",
      "\n",
      "One day, while playing a game with my friends, I found a strange message embedded in the code. I immediately shared it with my friends and we spent hours trying to decipher it. After several failed attempts, we decided to seek help from an expert. After several conversations and emails, we finally got in touch with Dr. Smith, a renowned computer scientist and cryptography expert.\n",
      "\n",
      "Dr. Smith was intrigued by our discovery and offered to help us decode the message. He explained that the message was written in a highly encrypted language and that it would take days or even weeks to decrypt it without the right key. However, he also mentioned that there was a possibility of it being an algorithmic puzzle designed to challenge our coding skills and creativity.\n",
      "\n",
      "We were thrilled at the prospect of solving such a challenging problem, and so we set out on the task with renewed vigor. We spent countless hours poring over the code, experimenting with different algorithms and techniques, and finally, after several days of hard work, we managed to crack the code and unravel\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"My name is Srushanth, I am \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arrr! Yer problem be solved by me, matey! Firstly, ye must gather yer crew and prepare for battle. Then, approach the llama slowly but with a fierce gaze. Use yer most dangerous pirate weapon - yer cutlass or pistol! Aye, these can do the trick. If it fights back, be prepared to use yer parrot squawk and distract it while ye strike. Finally, once the llama is defeated, claim its spoils as yer own! Yippee!\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"[INST] You are a pirate chatbot who always responds with Arr and pirate speak! There's a llama on my lawn, how can I get rid of him? [/INST]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```python\n",
      "def greet(name):\n",
      "    \"\"\"Function to greet the user\"\"\"\n",
      "    message = f\"Hello, {name}! Welcome to my world.\"\n",
      "    print(message)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"[INST] Write a sample python function. [/INST]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"My sample Gradio Mistral AI App\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"This is my description.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    [\"[INST] I want to learn Machine Learning. Can you please help me? [/INST]\"], \n",
    "    [\"[INST] I want to learn to swim. [/INST]\"], \n",
    "    [\"[INST] I want to save money. [/INST]\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\utils.py:840: UserWarning: Expected 1 arguments for function <ctransformers.llm.LLM object at 0x000002601C61E750>, received 2.\n",
      "  warnings.warn(\n",
      "c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\utils.py:848: UserWarning: Expected maximum 1 arguments for function <ctransformers.llm.LLM object at 0x000002601C61E750>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 455, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1533, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1151, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: LLM.__call__() takes 2 positional arguments but 3 were given\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 455, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1533, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1151, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: LLM.__call__() takes 2 positional arguments but 3 were given\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 493, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\GitHub\\Intellithing-Mistral-AI\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 464, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: None\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(llm, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(llm, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "gr.close_all()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demp = gr.Interface(fn=llm, inputs=\"text\", outputs=\"markdown\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_response(_test):\n",
    "    return \"Hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Hi [/INST]\n",
      " Hello! How can I help you today?\n",
      "****************************************\n",
      "[INST] Hi [/INST]\n",
      " Hello! How can I help you today?\n",
      "[INST] My name is Srushanth [/INST]\n",
      " That's a great name, Srushanth! Is there anything specific you would like to talk about or ask me? I'm here to assist with any information you need.\n",
      "****************************************\n",
      "[INST] Hi [/INST]\n",
      " Hello! How can I help you today?\n",
      "[INST] My name is Srushanth [/INST]\n",
      " That's a great name, Srushanth! Is there anything specific you would like to talk about or ask me? I'm here to assist with any information you need.\n",
      "[INST] What is my name? [/INST]\n",
      " Your name is Srushanth.\n",
      "****************************************\n",
      "[INST] Hi [/INST]\n",
      " Hello! How can I help you today?\n",
      "[INST] My name is Srushanth [/INST]\n",
      " That's a great name, Srushanth! Is there anything specific you would like to talk about or ask me? I'm here to assist with any information you need.\n",
      "[INST] What is my name? [/INST]\n",
      " Your name is Srushanth.\n",
      "[INST] Write a simple python function [/INST]\n",
      " Sure, here is an example of a simple Python function that takes in two numbers and returns their sum:\n",
      "```python\n",
      "def add_numbers(x, y):\n",
      "  result = x + y\n",
      "  return result\n",
      "\n",
      "a = 5\n",
      "b = 7\n",
      "sum_result = add_numbers(a, b)\n",
      "print(f\"The sum of {a} and {b} is {sum_result}\")\n",
      "```\n",
      "This function takes in two parameters, `x` and `y`, which represent the two numbers to be added. It then calculates their sum using the `+` operator and stores the result in a variable called `result`. Finally, it returns this value using the `return` statement.\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (513) exceeded maximum context length (512).\n",
      "Number of tokens (514) exceeded maximum context length (512).\n",
      "Number of tokens (515) exceeded maximum context length (512).\n",
      "Number of tokens (516) exceeded maximum context length (512).\n",
      "Number of tokens (517) exceeded maximum context length (512).\n",
      "Number of tokens (518) exceeded maximum context length (512).\n",
      "Number of tokens (519) exceeded maximum context length (512).\n",
      "Number of tokens (520) exceeded maximum context length (512).\n",
      "Number of tokens (521) exceeded maximum context length (512).\n",
      "Number of tokens (522) exceeded maximum context length (512).\n",
      "Number of tokens (523) exceeded maximum context length (512).\n",
      "Number of tokens (524) exceeded maximum context length (512).\n",
      "Number of tokens (525) exceeded maximum context length (512).\n",
      "Number of tokens (526) exceeded maximum context length (512).\n",
      "Number of tokens (527) exceeded maximum context length (512).\n",
      "Number of tokens (528) exceeded maximum context length (512).\n",
      "Number of tokens (529) exceeded maximum context length (512).\n",
      "Number of tokens (530) exceeded maximum context length (512).\n",
      "Number of tokens (531) exceeded maximum context length (512).\n",
      "Number of tokens (532) exceeded maximum context length (512).\n",
      "Number of tokens (533) exceeded maximum context length (512).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Hi [/INST]\n",
      " Hello! How can I help you today?\n",
      "[INST] My name is Srushanth [/INST]\n",
      " That's a great name, Srushanth! Is there anything specific you would like to talk about or ask me? I'm here to assist with any information you need.\n",
      "[INST] What is my name? [/INST]\n",
      " Your name is Srushanth.\n",
      "[INST] Write a simple python function [/INST]\n",
      " Sure, here is an example of a simple Python function that takes in two numbers and returns their sum:\n",
      "```python\n",
      "def add_numbers(x, y):\n",
      "  result = x + y\n",
      "  return result\n",
      "\n",
      "a = 5\n",
      "b = 7\n",
      "sum_result = add_numbers(a, b)\n",
      "print(f\"The sum of {a} and {b} is {sum_result}\")\n",
      "```\n",
      "This function takes in two parameters, `x` and `y`, which represent the two numbers to be added. It then calculates their sum using the `+` operator and stores the result in a variable called `result`. Finally, it returns this value using the `return` statement.\n",
      "[INST] Please explain the code for me. [/INST]\n",
      " The Python function shown above is an implementation of a simple addition operation. Here's a step-by-step explanation of the code:\n",
      "```python\n",
      "def add_numbers(x, y):  # Define the function\n",
      "    result = x + y  # Calculate the sum of the two input numbers\n",
      "    return result  # Return the result\n",
      "```\n",
      "The `add_numbers` function takes in two parameters (`x` and `y`) which represent the two input numbers. Inside the function, we calculate their sum using the `+` operator and store it in a variable called `result`. Finally, the `return` statement is used to return the value of `result`, which represents the sum of the two input numbers.\n",
      "```python\n",
      "a = 5  # Assign the first number to a variable\n",
      "b = 7  # Assign the second number to another variable\n",
      "sum_result = add_numbers(a, b)  # Call the function and assign its return value to a new variable called 'sum_result'\n",
      "print(f\"The sum of {a} and } and } and } and } and } and } and } and } and {a}\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (543) exceeded maximum context length (512).\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "while True:\n",
    "    _input = input(\"Message \")\n",
    "    prompt += f\"[INST] {_input} [/INST]\"\n",
    "    response = llm(prompt)\n",
    "    print(prompt)\n",
    "    print(response)\n",
    "    prompt += f\"\\n{response}\\n\"\n",
    "    print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\GitHub\\\\Intellithing-Mistral-AI\\\\model_gguf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.abspath(\"../model_gguf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
